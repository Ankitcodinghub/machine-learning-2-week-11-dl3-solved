# machine-learning-2-week-11-dl3-solved
**TO GET THIS SOLUTION VISIT:** [Machine Learning 2 Week 11-DL3 Solved](https://www.ankitcodinghub.com/product/machine-learning-2-week-11-dl3-solved/)


---

üì© **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
üì± **WhatsApp:** +1 419 877 7882  
üìÑ **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;98849&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;0&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;0&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;0\/5 - (0 votes)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;Machine Learning 2 Week 11-DL3 Solved&quot;,&quot;width&quot;:&quot;0&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 0px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            <span class="kksr-muted">Rate this product</span>
    </div>
    </div>
<div class="page" title="Page 1">
<div class="layoutArea">
<div class="column"></div>
<div class="column">
Exercise Sheet 11

</div>
</div>
<div class="layoutArea">
<div class="column">
Exercise 1: Activation Maximization (20 P)

Consider the linear model f(x) = w‚ä§x + b mapping some input x to an output f(x). We would like to interpret the function f by building a prototype x‚ãÜ in the input domain which produces a large value f. Activation maximization produces such interpretation by optimizing

max Ùè∞áf (x) ‚àí Œ©(x)Ùè∞à. x

<ol>
<li>(a) &nbsp;Find the prototype x‚ãÜ obtained by activation maximization subject to the penalty Œ©(x) = Œª‚à•x‚à•2.</li>
<li>(b) &nbsp;Find the prototype x‚ãÜ obtained by activation maximization subject to the penalty Œ©(x) = ‚àí log p(x) with
x ‚àº N (Œº, Œ£) where Œº and Œ£ are the mean and covariance.
</li>
<li>(c) &nbsp;Find the prototype x‚ãÜ obtained when the data is generated as (i) z ‚àº N (0, I) and (ii) x = Az + c, with A and c the parameters of the generator. Here, we optimize f w.r.t. the code z subject to the penalty Œ©(z) = Œª‚à•z‚à•2.
Exercise 2: Layer-Wise Relevance Propagation (30 P)

We would like to test the dependence of layer-wise relevance propagation (LRP) on the structure of the neural network. For this, we consider the function y = min(a1,a2), where a1,a2 ‚àà R+ are the input activations. This function can be implemented as a ReLU network in multiple ways. Two examples are given below.

(i) a 1 a (ii) a -1 a

1 31 1 3-1
</li>
</ol>
</div>
</div>
<div class="layoutArea">
<div class="column">
1y

a2 -1 a4 -1 a2 1 a4 1

<ol>
<li>(a) &nbsp;Show that these two networks implement the ‚Äòmin‚Äô function on the relevant domain.</li>
<li>(b) &nbsp;We consider the LRP-Œ≥ propagation rule:</li>
</ol>
</div>
</div>
<div class="layoutArea">
<div class="column">
1y

</div>
</div>
<div class="layoutArea">
<div class="column">
Ùè∞Ñ aj¬∑(wjk+Œ≥w+) jk

</div>
</div>
<div class="layoutArea">
<div class="column">
Rj = Ùè∞É aj ¬∑(wjk +Œ≥w+)Rk kj jk

</div>
</div>
<div class="layoutArea">
<div class="column">
where ()+ denotes the positive part. For each network, give for the case a1 = a2 an analytic solution for the scores R1 obtained by application this propagation rule at each layer. More specifically, express R1 as a function of the input activations.

Exercise 3: Neuralization (20 P)

Consider the one-class SVM that predicts for every new data point x the ‚Äòinlierness‚Äô score:

M

f(x) = Ùè∞ÑŒ±ik(x,ui)

i=1

where (ui)Mi=1 is the collection of support vectors, and Œ±i &gt; 0 are their weightings. We use the Gaussian

kernel k(x, x‚Ä≤) = exp(‚àíŒ≥‚à•x ‚àí x‚Ä≤‚à•2).

Because we are typically interested in the degree of anomaly of a particular data point, we can also define the score o(x) = ‚àí Œ≥1 log f (x) which grows with the degree of anomaly of the data point.

</div>
</div>
</div>
<div class="page" title="Page 2">
<div class="layoutArea">
<div class="column">
(a) Show that the outlier score o(x) can be rewritten as a two-layer neural network:

hi = ‚à•x ‚àí ui‚à•2 ‚àí Œ≥‚àí1 log Œ±i (layer 1)

o(x) = ‚àíŒ≥1 log Ùè∞ÉMi=1 exp(‚àíŒ≥hi) (layer 2)

(b) Show that the layer 2 converges to a min-pooling (i.e. o(x) = minNi=1{hi}) in the limit of Œ≥ ‚Üí ‚àû.

Exercise 4: Programming (30 P)

Download the programming files on ISIS and follow the instructions.

</div>
</div>
</div>
<div class="page" title="Page 3">
<div class="section">
<div class="section">
<div class="layoutArea">
<div class="column">
Exercise sheet 11 (programming) [SoSe 2021] Machine Learning 2

</div>
</div>
<div class="layoutArea">
<div class="column">
Explaining the Predictions of the VGG-16 Network

In this homework, we would like to implement different explanation methods on the VGG-16 network used for image classification. As a test example, we take some image of a castle

</div>
</div>
<div class="layoutArea">
<div class="column">
and would like to explain why the VGG:16 neuron castle activates for this image. The code below loads the image and normalizes

</div>
</div>
<div class="layoutArea">
<div class="column">
it, loads the model, and identifies the output neuron corresponding to the class

</div>
<div class="column">
castle .

</div>
</div>
<div class="layoutArea">
<div class="column">
In [1]:

</div>
<div class="column">
<pre>import numpy
import cv2
import torch
import torch.nn
import utils
</pre>
# Load image

</div>
</div>
<div class="layoutArea">
<div class="column">
<pre>img = numpy.array(cv2.imread('castle.jpg'))[...,::-1]/255.0
mean = torch.Tensor([0.485, 0.456, 0.406]).reshape(1,-1,1,1)
std  = torch.Tensor([0.229, 0.224, 0.225]).reshape(1,-1,1,1)
X = (torch.FloatTensor(img[numpy.newaxis].transpose([0,3,1,2])*1) - mean) / std
</pre>
<pre># Load VGG-16 network
</pre>
import torchvision

model = torchvision.models.vgg16(pretrained=True); model.eval();

<pre># Identify neuron "castle"
</pre>
cl = 483

</div>
</div>
<div class="layoutArea">
<div class="column">
Gradient √ó Input (15 P)

A simple method for explanation is Gradient √ó Input. Denoting f the function corresponding to the activation of the desired output neuron, and x the data point for which we would like to explain the prediction, the method assigns feature relevance using the formula

Ri = [‚àáf(x)]i ‚ãÖ xi

for all i = 1 ‚Ä¶ d. When the neural network is piecewise linear and positively homogeneous, the method delivers the same result as

one would get with a Taylor expansion at reference point x ÃÉ = Œµ ‚ãÖ x with Œµ almost zero. Task:

Implement Gradient √ó Input, i.e. write a function that produces a tensor of same dimensions as the input data and that contains the scores (Ri)i, test it on the given input image, and visualize the result using the function

utils.visualize .

</div>
</div>
</div>
</div>
</div>
<div class="page" title="Page 4">
<div class="section">
<div class="layoutArea">
<div class="column">
In [2]: %matplotlib inline

</div>
</div>
<div class="layoutArea">
<div class="column">
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì # TODO: Replace by your code

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì import solution solution.gradinput(model,X,cl)

<pre># --------------------------------------
</pre>
</div>
</div>
<div class="layoutArea">
<div class="column">
We observe that the explanation is noisy and has a large amount of positive and negative evidence. To produce a more robust explanation, we would like to use instead LRP, and implement it using the trick described in the slides. The trick consists of detaching certain terms from the differentiation graph, so that the explanation can obtained by performing Gradient √ó Input with the modified gradient.

LRP rules are typically set different at each layer. In VGG-16, we distinguish the following three types of layers of parameters:

First convolution: This convolution receives pixels as input, and it requires a special rewrite rule that we given below.

Next convolutions: All remaining convolutions receives activations as input, and we can treat them using LRP-Œ≥.

Dense layers: For these layers, we want to let the standard gradient signal flow, so we can simply leave these layers intact.

LRP in First Convolution

This convolution implements the propagation rule:

xw ‚àílw+‚àíhw‚àí R=‚àë iij iij iij R

i ‚àë xiwij ‚àíliw+ ‚àíhiw‚àí j j0,i ijij

where (‚ãÖ)+ and (‚ãÖ)‚àí are shortcut notations for max(0, ‚ãÖ) and min(0, ‚ãÖ), xi the value of pixel i, and where li and hi denote lower

and upper-bounds on pixel values. To implement this rule using the proposed approach, we define the quantities

pj = xi wij ‚àí li w+ ‚àí hi w‚àí and zj = xi wij and perform the reassignation:

zj ‚Üêpj ‚ãÖ[zj/pj]cst.

This is done by the code below

In [3]: class ConvPx(torch.nn.Module): def __init__(self, conv):

torch.nn.Module.__init__(self)

self.conv = conv

self.pconv = utils.newlayer(conv, lambda p: p.clamp(min=0)) self.nconv = utils.newlayer(conv, lambda p: p.clamp(max=0))

def forward(self, X): X, L, H = X

z = self.conv.forward(X)

zp = z ‚Äì self.pconv.forward(L) ‚Äì self.nconv.forward(H) return zp * (z / zp).data

</div>
</div>
<div class="layoutArea">
<div class="column">
ij ij

</div>
</div>
</div>
</div>
<div class="page" title="Page 5">
<div class="section">
<div class="layoutArea">
<div class="column">
LRP in Next Convolutions (15 P)

</div>
</div>
<div class="layoutArea">
<div class="column">
In the next convolutions, we would like to apply the LRP-Œ≥ rule given by:

R=‚àë aj(wjk+Œ≥w+) R

</div>
</div>
<div class="layoutArea">
<div class="column">
jk

j ‚àë aj(wjk+Œ≥w+) k

</div>
</div>
<div class="layoutArea">
<div class="column">
k0,j jk

To implement this rule using the proposed approach, we define the quantities pk = aj(wjk + Œ≥w+ ) and zk = ajwjk and

</div>
</div>
<div class="layoutArea">
<div class="column">
perform the reassignation:

</div>
</div>
<div class="layoutArea">
<div class="column">
zk ‚Üêpk ‚ãÖ[zk/pk]cst.

Inspired by the code for the first convolution, implement a class for the next convolutions that is equiped to perform the

</div>
</div>
<div class="layoutArea">
<div class="column">
Task:

LRP-Œ≥ propagation when calling the gradient.

In [4]: class Conv(torch.nn.Module):

def __init__(self, conv, gamma):

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì # TODO: Replace by your code

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì import solution solution.convinit(self,conv,gamma)

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

def forward(self, X):

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì # TODO: Replace by your code

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì import solution

return solution.convforward(self,X)

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì

Now, we can create the LRP-enabled model by replacing the convolution layers with their modified versions.

</div>
</div>
<div class="layoutArea">
<div class="column">
jk

</div>
</div>
<div class="layoutArea">
<div class="column">
In [5]:

Note that for the layer 0 and for the subsequent layers, we have used two different classes. Also, the parameter Œ≥ is set high in the lower layers and goes increasingly closer to zero as we reach the top convolutions.

We then proceed like Gradient √ó Input, except for the fact that in the modified first convolution the inputs and gradients not only comprise the pixels x but also the lower and upper bounds l and h. The code below implements this extended Gradient √ó Input and visualizes the resulting explanation.

</div>
</div>
<div class="layoutArea">
<div class="column">
lrpmodel = torchvision.models.vgg16(pretrained=True); model.eval() features = lrpmodel._modules[‚Äòfeatures‚Äô]

</div>
</div>
<div class="layoutArea">
<div class="column">
for i in [0]:

for i in [2]:

for i in [5,7]:

for i in [10,12,14]: features[i] = Conv(features[i],10**(-1.0)) for i in [17,19,21]: features[i] = Conv(features[i],10**(-1.5)) for i in [24,26,28]: features[i] = Conv(features[i],10**(-2.0))

</div>
</div>
<div class="layoutArea">
<div class="column">
<pre>features[i] = ConvPx(features[i])
features[i] = Conv(features[i],10**(-0.0))
features[i] = Conv(features[i],10**(-0.5))
</pre>
</div>
</div>
</div>
</div>
<div class="page" title="Page 6">
<div class="layoutArea">
<div class="column">
In [6]: # Prepare data and lower/upper bounds X.grad = None

L = (X*0+((0-mean)/std).view(1,3,1,1)).data H = (X*0+((1-mean)/std).view(1,3,1,1)).data X.requires_grad_(True) L.requires_grad_(True) H.requires_grad_(True)

<pre>        # Apply the modfied gradient x input
</pre>
<pre>        lrpmodel.forward((X,L,H))[0,cl].backward()
        R = (X*X.grad+L*L.grad+H*H.grad)
</pre>
%matplotlib inline utils.visualize(R)

</div>
</div>
<div class="layoutArea">
<div class="column">
We observe that most of the noise has disappeared, and we can clearly see in red which patterns in the data the neural network has used to predict ‚Äòcastle‚Äô. We also see in blue which patterns contribute negatively to that class, e.g. the corner of the roof and the traffic sign.

</div>
</div>
</div>
<div class="page" title="Page 7"></div>
<div class="page" title="Page 8"></div>
